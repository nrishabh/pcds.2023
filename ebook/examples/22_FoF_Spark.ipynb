{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce265c4c-a998-41f8-a413-2919cde7edc3",
   "metadata": {},
   "source": [
    "## PySpark Friends of Friends\n",
    "\n",
    "We are going to reproduce the Friends of Friends Hadoop example in Spark.\n",
    "First things first, let's run it as a map/reduce program in Spark.\n",
    "\n",
    "Node A with neighbors B and C propose candidate triples to it's neighbors\n",
    "* B A C to node B (A<C, else B C A)\n",
    "* C A B to node C (A<B else C B A)\n",
    "\n",
    "All triples will get two proposals from it's neighbors and reduce them. If there are two matching proposals, we have a triple.\n",
    "\n",
    "#### Map/Reduce Implementation\n",
    "\n",
    "The code below `lines_to_triples` implements the proposal to be run in `flatMap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb9eca2-47cd-4201-b6ed-6c4c8ce93d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def line_to_triples(line: str):\n",
    "    fids = np.array(line.split(), dtype=int)\n",
    "    ret = []\n",
    "    for i in range(1, len(fids) - 1):\n",
    "        for j in  range(i + 1, len(fids)):\n",
    "            source = fids[0]\n",
    "            fi, fj = fids[i], fids[j]\n",
    "            if source < fi:\n",
    "                ret.append([fj, source, fi])\n",
    "            else:\n",
    "                ret.append([fj, fi, source])\n",
    "            if source < fj:\n",
    "                ret.append([fi, source, fj])\n",
    "            else:\n",
    "                ret.append([fi, fj, source])\n",
    "    return ret    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b704dc7-6fbf-42a2-bc7c-7c7442c62f5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Simple test to show what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3741e37d-be14-4f54-a983-25001937827e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 1, 5],\n",
       " [5, 1, 8],\n",
       " [7, 1, 5],\n",
       " [5, 1, 7],\n",
       " [9, 1, 5],\n",
       " [5, 1, 9],\n",
       " [7, 1, 8],\n",
       " [8, 1, 7],\n",
       " [9, 1, 8],\n",
       " [8, 1, 9],\n",
       " [9, 1, 7],\n",
       " [7, 1, 9]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_to_triples (\"1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a99c61-970d-4cbf-9bb6-c6d9bd599b54",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Now, complete the program. You will have to use the wordcount style `<triple>, 1` to get a simpler reducer to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc876d3a-caa7-4ba6-a55c-2869fb36eae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/15 22:08:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/outputsimple.mre\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "### TODO add a WordCount style reducer\n",
    "rdd = rdd.map(lambda x: (str(x), 1))\n",
    "rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "### TODO Identify triples of count >=2\n",
    "rdd = rdd.filter(lambda x: x[1]>1)\n",
    "### TODO format output to just triples\n",
    "rdd = rdd.map(lambda x: x[0])\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e1192af-571a-4b2a-a2b6-46f72e7c4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e81ac7-27ec-498f-ac24-c26dc4ee38f2",
   "metadata": {},
   "source": [
    "#### Join Implementation\n",
    "\n",
    "In previous years when we ran this on distributed memory, we found that an implementation using a `join` was faster. This is because join has efficient implementation (based on a hash map) that do not have to send all the triples around. The concept of the program is:\n",
    "* output all triples (same as before)\n",
    "* add an partition number to each triple\n",
    "* perform a self-join on the triples with partition numbers\n",
    "* use the partition index to identify when the triples came from different lists\n",
    "    * self-join will produce joined triples from the same partition \n",
    "* output triples when two proposals came from different partitions\n",
    "    * i.e. two proposal from different friends lists\n",
    "    * you need to avoid additional output when there is no match\n",
    "    \n",
    "This is a pretty different usage of Spark.  The function `mapPartitions` allows the programmer to write functions that apply to an entire partition, rather than each individual element of an RDD.  The function `mapPartitionsWithIndex` makes the partition identifier available to differentiate behavior, analagous to have the thread ID in OpenMP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771e48f4-7be1-4e9e-8516-6c8e25b1073b",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following helper function will add an index to the triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f8a9865-d765-4935-ab73-a2d1aecef851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### add a parition index to each triple.\n",
    "def add_index (idx, part):\n",
    "    for p in part:\n",
    "        yield str(p), str(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68537dc4-8182-4533-960d-ede8f5df6309",
   "metadata": {
    "tags": []
   },
   "source": [
    "You will need to write a function to identify when the joined triples have different indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7638ab0-0b93-4131-87e2-a2931749ba37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_diff_idx (x):\n",
    "    \n",
    "    idxs = set()\n",
    "    for i in x[1]:\n",
    "        idxs.add(i)\n",
    "    if len(idxs) > 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c33970c5-0107-45cc-b78f-fcd692bf4eba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/output.join\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "rdd = rdd.mapPartitionsWithIndex(add_index)\n",
    "# TODO self join\n",
    "rdd = rdd.join(rdd)\n",
    "# TODO filter out matches from same partition\n",
    "rdd = rdd.filter(lambda x: x[1][0] != x[1][1])\n",
    "# TODO cleanup output\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61753d5-ddd1-4dcc-915c-de554e0054a7",
   "metadata": {},
   "source": [
    "The following `sc.stop()` can be called alone to clean up crashed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04ef2a6-420d-43d6-ae0c-734a324c03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482b323-0976-4e1b-b069-f60b7741a1b0",
   "metadata": {},
   "source": [
    "It turns out (we will see below) that this version is not faster on shared-memory. It does a lot more computation and the reduction in traffic is not as important, because memory is faster than networking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871b123-cd3b-4619-8f8e-2af4cf40c3f3",
   "metadata": {},
   "source": [
    "### List Merge Version\n",
    "\n",
    "A better idea might be to use the memory of Spark to make things faster. We are going to make the assumption that each Spark worker can hold two entire friends lists at once.  So this idea here is:\n",
    "* merge pairs of lists representing, e.g. from 100 and 200, into an RDD. You should generate an RDD with entries like:\n",
    "    * `'100, 200', array([300, 319, 400])` from 100  \n",
    "    * `'100, 200', array([219, 300, 400])` from 200\n",
    "    * note that we sorted friends in the key so that the keys will match.\n",
    "    * also note that list only contains the remaining friends, i.e. not 100 or 200\n",
    "* group these lists together\n",
    "    * '100, 200', [array([219, 300, 400]), array([300, 319, 400])]`\n",
    "* when you have two arrays in the value, compute their intersection\n",
    "    * '100, 200', [300, 400]\n",
    "* output the corresponding triples\n",
    "    * be careful here. it's subtle to get the output right.\n",
    "    \n",
    "You have write any helper functions that you need. I'm including prototypes for the ones I used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae4e7c60-1961-4657-89e1-1ad0038fb77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the remaining list of friends for each friend.\n",
    "def pair_lists(line: str):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c067377a-383d-41e4-a772-3fb09b50a8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pair_lists (\"2 1 5 8 7 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7fb23-6006-41f8-add1-59f8e220573d",
   "metadata": {},
   "source": [
    "Sample output from pair_list\n",
    "```\n",
    "[['1, 2', array([5, 8, 7, 9])],\n",
    " ['2, 5', array([1, 8, 7, 9])],\n",
    " ['2, 8', array([1, 5, 7, 9])],\n",
    " ['2, 7', array([1, 5, 8, 9])]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff86f224-8844-410e-97ef-6777b9110c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Output the appropriate triples after intersection.\n",
    "def output_triples(x):\n",
    "    output = []\n",
    "    # TODO\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c99bb673-9690-4530-93f3-00e96bf2d09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46febb59-f3d0-4ba2-a71b-17374d213811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/15 22:08:58 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/11/15 22:08:58 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (991fc5c4f86c executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "23/11/15 22:08:58 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n",
      "23/11/15 22:08:58 ERROR SparkHadoopWriter: Aborting job job_202311152208571761171388012653387_0008.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (991fc5c4f86c executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2316)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n",
      "    for k, v in iterator:\n",
      "TypeError: 'NoneType' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "23/11/15 22:08:58 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (991fc5c4f86c executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o164.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (991fc5c4f86c executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2316)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/PCDS/ebook/examples/22_FoF_Spark.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f50434453222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f47726164253230537475646965732f46616c6c253230323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/PCDS/ebook/examples/22_FoF_Spark.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f50434453222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f47726164253230537475646965732f46616c6c253230323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/PCDS/ebook/examples/22_FoF_Spark.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m rdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mflatMap(output_triples)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f50434453222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f47726164253230537475646965732f46616c6c253230323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/PCDS/ebook/examples/22_FoF_Spark.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m rdd\u001b[39m.\u001b[39;49msaveAsTextFile(outdir)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f50434453222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f47726164253230537475646965732f46616c6c253230323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f726973686162682f4c6962726172792f436c6f756453746f726167652f4f6e6544726976652d4a6f686e73486f706b696e732f4772616420537475646965732f46616c6c20323032332f504344532f2e646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/PCDS/ebook/examples/22_FoF_Spark.ipynb#Y101sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m sc\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py:3406\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   3404\u001b[0m     keyed\u001b[39m.\u001b[39m_jrdd\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mBytesToString())\u001b[39m.\u001b[39msaveAsTextFile(path, compressionCodec)\n\u001b[1;32m   3405\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3406\u001b[0m     keyed\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mmap(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mBytesToString())\u001b[39m.\u001b[39;49msaveAsTextFile(path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o164.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (991fc5c4f86c executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2316)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n\t... 51 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n    process()\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 820, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 5405, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 828, in func\n    return f(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/rdd.py\", line 4156, in combine\n    merger.mergeValues(iterator)\n  File \"/opt/conda/lib/python3.10/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 256, in mergeValues\n    for k, v in iterator:\nTypeError: 'NoneType' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/simple.input\"\n",
    "outdir = \"/tmp/output.merge\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(pair_lists)\n",
    "# group by and turn into lists\n",
    "rdd = rdd.groupByKey().mapValues(list)\n",
    "# keep only keys with two inputs\n",
    "rdd = rdd.filter(lambda x: len(x[1])==2)\n",
    "# TODO intersect the two lists\n",
    "...\n",
    "rdd = rdd.flatMap(output_triples)\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b40b05-8fe1-42db-937e-bc2b5c7ad964",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timings\n",
    "\n",
    "Run on the full dataset to compare performance.\n",
    "\n",
    "I've removed the code, but I've left the timing information for your reference.\n",
    "  * M/R OK\n",
    "  * Join slowest\n",
    "  * Merge fastest\n",
    "Conclusion: different implementations better for different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56ceff-a675-46e6-aef8-6cf1364e7f34",
   "metadata": {},
   "source": [
    "##### Map/Reduce version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9370bc58-8032-4877-a9ac-cd6e9b39aff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.mr\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30103e99-127b-4a1e-b29a-3010cd8cc0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Join version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b904e2-cf58-49fc-ba45-63205d15742c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.join\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "rdd = rdd.flatMap(line_to_triples)\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfee4c5-1fe6-41ae-ac92-bac4a41b0792",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### List merge version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54c01ad-698f-4f05-ba77-e2582f18f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "inputdir = \"../data/fof.input\"\n",
    "outdir = \"/tmp/outputfof.merge\"\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"App Name\",)\n",
    "rdd = sc.textFile(f\"{inputdir}/*\")\n",
    "...\n",
    "rdd.saveAsTextFile(outdir)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9016a66-7fb1-4eee-8be8-4a45242ac468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
